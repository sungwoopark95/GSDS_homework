{"cells":[{"cell_type":"markdown","metadata":{"id":"EDxtMC2g66gQ"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22317,"status":"ok","timestamp":1655452443592,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"},"user_tz":-540},"id":"vj2CXov7JJqq","outputId":"b72fee57-74af-4cfe-c1ad-860f9dc22ebb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DcKp4bZiJwut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655452443593,"user_tz":-540,"elapsed":8,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}},"outputId":"b3a1516c-69eb-4b5a-f13e-b69a2aa3dcc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: './drive/My Drive/mldl/DL_Lab4'\n","/content\n"]}],"source":["%cd ./drive/My Drive/mldl/DL_Lab4"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UHKo6dP6eEO6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655452456210,"user_tz":-540,"elapsed":12622,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}},"outputId":"8eae4691-6e23-4b3c-8608-53aa72cc20ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 14.6 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 50.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 70.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 6.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.0\n"]}],"source":["!pip install transformers\n","import math\n","import pickle\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","import pandas as pd\n","import numpy as np\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"OOnSsL1EeG85","executionInfo":{"status":"ok","timestamp":1655452456210,"user_tz":-540,"elapsed":20,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"outputs":[],"source":["SEED = 1111\n","\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"id":"5PgCsIyawXoN"},"source":["## Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"4Lvt4lGJIe8i","colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"status":"error","timestamp":1655452456218,"user_tz":-540,"elapsed":27,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}},"outputId":"d6511dec-2601-4337-a796-6af79ded2470"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-0dd6f826a780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from pathlib import Path\n","import sys\n","sys.path.insert(0,str(Path().absolute().joinpath(\"data\")))\n","from data import prepareData\n","import random\n","random.seed(SEED)\n","from torch.utils.data import random_split\n","\n","SOS_token = 0\n","EOS_token = 1\n","\n","MAX_LENGTH = 10\n","\n","TRAIN_RATIO = 0.6\n","VALID_RATIO = 0.2\n","\n","# BATCH_SIZE = 64\n","BATCH_SIZE = 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fePBsU2GKoaI","executionInfo":{"status":"aborted","timestamp":1655452456210,"user_tz":-540,"elapsed":16,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"outputs":[],"source":["class TranslateDataset(Dataset):\n","    def __init__(self, max_length=10, fra2eng=True):\n","        self.input_lang, self.output_lang, self.pairs = prepareData('eng', 'fra', max_length=max_length, reverse=fra2eng)\n","        self.max_length=max_length\n","\n","        self.input_lang.addWord('PAD')\n","        self.output_lang.addWord('PAD')\n","        self.input_lang_pad = self.input_lang.word2index['PAD']\n","        self.output_lang_pad = self.output_lang.word2index['PAD']\n","        \n","        print(\"data example\")\n","        print(random.choice(self.pairs))\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","        pair = self.pairs[idx]\n","        x, y = self._tensorsFromPair(pair)\n","        return x, y\n","\n","    def _tensorFromSentence(self, lang, sentence):\n","        indexes = [lang.word2index[word] for word in sentence.split(' ')]\n","        indexes.append(EOS_token)\n","        return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n","\n","    def _tensorsFromPair(self, pair):\n","        input_tensor = self._tensorFromSentence(self.input_lang, pair[0])\n","        target_tensor = self._tensorFromSentence(self.output_lang, pair[1])\n","        return (input_tensor, target_tensor)\n","    \n","    def collate_fn(self, data):\n","        x_batch = []; y_batch = []\n","        \n","        for x, y in data:\n","            if x.shape[0] < self.max_length-1:\n","                x = torch.cat([x, self.input_lang_pad*torch.ones((self.max_length-1 - x.shape[0], 1), dtype=x.dtype)])\n","            elif x.shape[0] > self.max_length-1:\n","                x = x[:self.max_length-1]\n","            if y.shape[0] < self.max_length-1:\n","                y = torch.cat([y, self.output_lang_pad*torch.ones((self.max_length-1 - y.shape[0], 1), dtype=y.dtype)])\n","            elif y.shape[0] > self.max_length-1:\n","                y = y[:self.max_length-1]\n","\n","            x_batch.append(torch.cat([torch.tensor([SOS_token]), x.squeeze(1)]))\n","            y_batch.append(torch.cat([torch.tensor([SOS_token]), y.squeeze(1)]))\n","        \n","        return torch.stack(x_batch), torch.stack(y_batch)\n","\n","dataset = TranslateDataset(max_length=MAX_LENGTH)\n","\n","train_size = int(len(dataset)*TRAIN_RATIO)\n","valid_size = int(len(dataset)*VALID_RATIO)\n","train_data, valid_data, test_data = random_split(dataset, [train_size, valid_size, len(dataset)-(train_size+valid_size)],)\n","\n","train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)\n","valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn, shuffle=True)"]},{"cell_type":"code","source":["sample_x, sample_y = next(iter(train_dataloader))\n","sample_x = sample_x.squeeze(0)\n","sample_y = sample_y.squeeze(0)\n","\n","print(\"Sample sentences\\n\")\n","print(\"sample_x: \", sample_x)\n","print(' '.join([dataset.input_lang.index2word[i] for i in sample_x.tolist()]))\n","print(\"sample_y: \", sample_y)\n","print(' '.join([dataset.output_lang.index2word[i] for i in sample_y.tolist()]))"],"metadata":{"id":"z6lIHD3Xf6Ik","executionInfo":{"status":"aborted","timestamp":1655452456211,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","    sample_x, sample_y = next(iter(train_dataloader))\n","    print(sample_x.item())"],"metadata":{"id":"oNP8O_HQs1cF","executionInfo":{"status":"aborted","timestamp":1655452456211,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop"],"metadata":{"id":"-YmgeTvBpxXL","executionInfo":{"status":"aborted","timestamp":1655452456211,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_x"],"metadata":{"id":"TghjcZVBs2tZ","executionInfo":{"status":"aborted","timestamp":1655452456211,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_x.size()"],"metadata":{"id":"fLOoNrNstNHn","executionInfo":{"status":"aborted","timestamp":1655452456211,"user_tz":-540,"elapsed":16,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_x.squeeze(0).size()"],"metadata":{"id":"jzvbK5NrtRuj","executionInfo":{"status":"aborted","timestamp":1655452456212,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_y.size()"],"metadata":{"id":"DR5lXyCx5GJM","executionInfo":{"status":"aborted","timestamp":1655452456212,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8TqutY58tG-h"},"source":["# 1. Seq2seq model with Attention Mechanism"]},{"cell_type":"code","source":["#############################\n","##### Initialize Model ######\n","#############################\n","\n","in_dim =  dataset.input_lang.n_words# french\n","out_dim =  dataset.output_lang.n_words # english\n","\n","emb_dim = 512 # embbeding size\n","hid_dim =  256 # vector size of encoder output\n","\n","print(f'\\nin_dim: {in_dim}\\tout_dim: {out_dim}\\temb_dim: {emb_dim}\\thid_dim: {hid_dim}\\n')"],"metadata":{"id":"rgIpHf60hRfY","executionInfo":{"status":"aborted","timestamp":1655452456212,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aFDRFkQTC3YQ"},"source":["## Encoder"]},{"cell_type":"code","source":["#############################\n","####### Prepare Input ####### \n","#############################\n","\n","print('Encoder Embedding outputs')\n","\n","embedded_x = nn.Embedding(in_dim, emb_dim)(sample_x)\n","print(embedded_x.shape)"],"metadata":{"id":"EbSJB9B-jl-4","executionInfo":{"status":"aborted","timestamp":1655452456212,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Initialize hidden and cell states')\n","\n","hidden_0 = torch.zeros(1,hid_dim) # (1, Hout) for unbatched input\n","cell_0 =  torch.zeros(1,hid_dim)# (1, Hcell) for unbatched input"],"metadata":{"id":"MEH1RRbe1DrA","executionInfo":{"status":"aborted","timestamp":1655452456212,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################\n","### Get output of Encoder ###\n","#############################\n"," \n","lstm = nn.LSTM(input_size=emb_dim, hidden_size=hid_dim)\n","hiddens, (hidden, cell) = lstm(embedded_x, (hidden_0, cell_0))\n","# hiddens = lstm(embedded_x, (hidden_0, cell_0))\n","\n","print('LSTM Encoder outputs')\n","print(f'hiddens: {hiddens.shape}\\thidden: {hidden.shape}\\tcell: {cell.shape}')\n","\n","enc_hiddens = hiddens # assign encoder outputs for decoder(w/attention, see below)"],"metadata":{"id":"XveBtVbhkMWp","executionInfo":{"status":"aborted","timestamp":1655452456213,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torch.sum(hiddens[-1]-hidden) # last value of hiddens = hidden"],"metadata":{"id":"iZhSxF7Kk89l","executionInfo":{"status":"aborted","timestamp":1655452456213,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m3jkpgL7C7-_"},"source":["## Decoder"]},{"cell_type":"code","source":["#############################\n","###### Prepare Output ####### \n","#############################\n","\n","print('Decoder Embedding outputs')\n","\n","dec_embedder = nn.Embedding(out_dim, emb_dim)\n","embedded_y = dec_embedder(sample_y) # ground truth\n","print(embedded_y.shape)"],"metadata":{"id":"kdho8e70mV6h","executionInfo":{"status":"aborted","timestamp":1655452456213,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################\n","####### Prepare Input ####### \n","#############################\n","\n","cell_0 = torch.zeros(1, hid_dim)\n","\n","input = embedded_y[0] # [SOS]\n","enc_output = hiddens[-1] # hidden\n","hidden = enc_output.unsqueeze(0)\n","cell = cell_0"],"metadata":{"id":"FHABukOMpY6u","executionInfo":{"status":"aborted","timestamp":1655452456213,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input.size()"],"metadata":{"id":"x7D9-oeOyyQ_","executionInfo":{"status":"aborted","timestamp":1655452456213,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enc_output.size()"],"metadata":{"id":"KwnemKp_u63N","executionInfo":{"status":"aborted","timestamp":1655452456213,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################\n","### Get output of Decoder ###\n","#############################\n","\n","seq2seq_outputs = []\n","decoder = nn.LSTM(input_size=emb_dim+hid_dim, hidden_size=hid_dim)\n","fc = nn.Linear(hid_dim, out_dim) # 인코더랑 달라지는 부분\n","\n","\n","#forward\n","for t in range(MAX_LENGTH):\n","\n","    input_encout_concat = torch.concat([input, enc_output]) \n","    \n","    hiddens, (hidden, cell) = decoder(input_encout_concat.unsqueeze(0), (hidden, cell))\n","    \n","    next_token_idx = F.softmax(fc(hidden)).max(1)[1]\n","    \n","    seq2seq_outputs.append(hiddens)\n","    \n","    # Update inputs for the next loop\n","    \n","    input = dec_embedder(next_token_idx).squeeze(0)\n","    # hidden = hidden\n","    # cell = cell\n","\n","    if t==0: \n","        print(f'input_encout_concat: {input_encout_concat.shape}')\n","        print('\\nLSTM Decoder outputs')\n","        print(f'hiddens: {hiddens.shape}\\thidden: {hidden.shape}\\tcell: {cell.shape}')\n","    "],"metadata":{"id":"l81Du2A-20uI","executionInfo":{"status":"aborted","timestamp":1655452456214,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seq2seq_outputs = torch.stack(seq2seq_outputs)\n","seq2seq_outputs.shape # predicted"],"metadata":{"id":"shLkhz5mv8TT","executionInfo":{"status":"aborted","timestamp":1655452456214,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Attention"],"metadata":{"id":"Tvzu6ONF2TyE"}},{"cell_type":"code","source":["#############################\n","## Set key/value and query ##\n","#############################\n","\n","# key, value\n","kv = enc_hiddens\n","print(\"Key/value shape:\\t\",  kv.shape)\n","\n","# query\n","example_t = 4 # any int [0 ~ MAX_LENGTH-1]\n","q = seq2seq_outputs[example_t]\n","print(\"Query shape:\\t\",  q.shape)"],"metadata":{"id":"DrmqlFUAw4b7","executionInfo":{"status":"aborted","timestamp":1655452456214,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################\n","#### Get attention value ####\n","#############################\n","\n","attn_score =  torch.matmul(kv,q.reshape(-1,1))\n","\n","attn_coefficient =  F.softmax(attn_score,dim=0)\n","\n","weighted_kv = kv*attn_coefficient\n","weighted_sum = torch.sum(weighted_kv, dim=0) # attention value\n","\n","print(\"attn_score shape:\\t\",  attn_score.shape)\n","print(\"weights: \\t\", attn_coefficient.squeeze(1).tolist())\n","print(\"total of weights: \\t\", sum(attn_coefficient.squeeze(1).tolist()))\n","print(\"weighted sum of val:\\t\", weighted_kv.shape)\n","print(\"weighted sum:\\t\", weighted_sum.shape)"],"metadata":{"id":"lSsUqg_8x4y-","executionInfo":{"status":"aborted","timestamp":1655452456214,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_idx = 4  # which index of encoder output to attend [0~9]\n","\n","print(\"before attn\")\n","print(sum(kv[sample_idx]))\n","print(\"attn weight\")\n","print(attn_coefficient[sample_idx])\n","print(\"after attn\")\n","print(sum(weighted_kv[sample_idx]))"],"metadata":{"id":"Mh495Oi1zwyP","executionInfo":{"status":"aborted","timestamp":1655452456214,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Seq2seq model with Transformer"],"metadata":{"id":"siPUPEFXEbrD"}},{"cell_type":"code","source":["in_dim = dataset.input_lang.n_words\n","out_dim = dataset.output_lang.n_words\n","emb_dim = 256"],"metadata":{"id":"F5yc8iJ9GFrJ","executionInfo":{"status":"aborted","timestamp":1655452456214,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################\n","########### Input ########### \n","#############################\n","\n","embedded_x = nn.Embedding(in_dim, emb_dim)(sample_x)\n","print(\"embedded_x:\\t\", embedded_x.shape)"],"metadata":{"id":"SZw7whv9GSdY","executionInfo":{"status":"aborted","timestamp":1655452456214,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Positional Encoding\n","1. Absolute sinusoid-based Positional Encoding\n","2. Relative Positional Encoding\n","3. Learnable Embedding (using nn.Embedding)"],"metadata":{"id":"sz_9Y4Q9Ecrt"}},{"cell_type":"code","source":["#############################\n","##### Position Encoding #####\n","#############################\n","\n","import matplotlib.pyplot as plt\n","\n","# Learnable\n","pos = torch.arange(MAX_LENGTH).unsqueeze(1)\n","pos_embedding = nn.Embedding(in_dim, emb_dim)(pos)\n","plt.pcolormesh(pos_embedding.squeeze(1).detach().numpy(), cmap='RdBu') # Learnable\n","\n","# Absolute\n","#with open('absolute_pe.pickle', 'rb') as handle:\n","#    pe = pickle.load(handle)\n","#plt.pcolormesh(pe.squeeze(1), cmap='RdBu')\n","\n","\n","plt.xlim((0, emb_dim))\n","plt.ylabel('Position')\n","plt.colorbar()\n","plt.show()\n"],"metadata":{"id":"xIg5RLUzEXQc","executionInfo":{"status":"aborted","timestamp":1655452456215,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedded_pos = embedded_x + pe.squeeze(1)\n","print(\"embedded_pos:\\t\",embedded_pos.shape)"],"metadata":{"id":"aiLvHDrm9MC4","executionInfo":{"status":"aborted","timestamp":1655452456215,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################\n","##### src * trg masking #####\n","#############################\n","\n","#Encoder: padding masking\n","#Decoder: padding, subsequent masking\n","\n","# source masking, \n","sample_x_mask = (sample_x == dataset.input_lang_pad) # input_lang_pad : [PAD]\n","\n","print(\"source mask = pad masking\")\n","print(sample_x_mask.squeeze().tolist())\n","\n","# target masking\n","pad_mask_neg = (sample_y != dataset.output_lang_pad).unsqueeze(1).unsqueeze(2)\n","sub_mask = torch.tril(torch.ones((MAX_LENGTH, MAX_LENGTH))).bool()\n","sample_y_mask = pad_mask_neg.permute(2,1,0) & sub_mask.unsqueeze(0)\n","\n","print(\"\\ntarget mask = pad masking + subsequent masking\")\n","print(pad_mask_neg.squeeze().tolist())\n","print()\n","print(sub_mask.squeeze())\n","print()\n","print(sample_y_mask.squeeze())"],"metadata":{"id":"GBizStE9POJR","executionInfo":{"status":"aborted","timestamp":1655452456215,"user_tz":-540,"elapsed":17,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" pad_mask_neg.permute(2,1,0)"],"metadata":{"id":"8rOl8Z9g3ys4","executionInfo":{"status":"aborted","timestamp":1655452456216,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#############################\n","########### Model ########### \n","#############################\n","\n","from torch.nn import Transformer\n","\n","hid_dim = emb_dim\n","ff_dim = 1024\n","n_heads = 8\n","n_enc_layers = 3\n","n_dec_layers = 5\n","dropout_p = 0.1\n","\n","class TransSeq2Seq(nn.Module):\n","    \n","    def __init__(self, hid_dim, ff_dim, n_heads, n_enc_layers, n_dec_layers, dropout_p):\n","        super().__init__()\n","        self.input_emb = nn.Embedding(in_dim, hid_dim)\n","        self.output_emb = nn.Embedding(out_dim, hid_dim)\n","        self.pos_emb = nn.Embedding(MAX_LENGTH, hid_dim)\n","\n","        self.transformer = Transformer(d_model=hid_dim,\n","                                       nhead=n_heads,\n","                                       num_encoder_layers=n_enc_layers,\n","                                       num_decoder_layers=n_dec_layers,\n","                                       dim_feedforward = ff_dim,\n","                                       dropout = dropout_p,\n","                                       activation = 'gelu')\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, src, trg):\n","        \n","        embedded_pos = self.pos_emb(torch.arange(MAX_LENGTH).unsqueeze(1))\n","        embedded_x = self.input_emb(src)\n","        embedded_y = self.output_emb(trg)\n","\n","        embedded_x = self.dropout(torch.sum(embedded_x + embedded_pos, dim=1))\n","        embedded_y = self.dropout(torch.sum(embedded_y + embedded_pos, dim=1))\n","\n","        return self.transformer(embedded_x, embedded_y)"],"metadata":{"id":"h-NgAbGqIFco","executionInfo":{"status":"aborted","timestamp":1655452456216,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = TransSeq2Seq(hid_dim, ff_dim, n_heads, n_enc_layers, n_dec_layers, dropout_p)\n","print(model)"],"metadata":{"id":"EljQjKn0JYkc","executionInfo":{"status":"aborted","timestamp":1655452456216,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out = model(sample_x, sample_y)\n","print(out.shape)"],"metadata":{"id":"3no_Y49MPA7_","executionInfo":{"status":"aborted","timestamp":1655452456217,"user_tz":-540,"elapsed":19,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZDma7XS2eKSQ"},"source":["# 3. Bert fine-tuning"]},{"cell_type":"markdown","metadata":{"id":"WaKMY_DfUDmC"},"source":["ref.\n","\n","1. [huggingface BERT documentation](https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bert)\n","\n","2. [masked language modelling with bert](https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c)\n","\n","3. [fine-tuning bert for text classification in pytorch](https://luv-bansal.medium.com/fine-tuning-bert-for-text-classification-in-pytorch-503d97342db2)\n","\n","4. [pytorch sentiment classification github](https://github.com/clairett/pytorch-sentiment-classification/tree/master/data/SST2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUQDOmLDeMPo","executionInfo":{"status":"aborted","timestamp":1655452456217,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"outputs":[],"source":["from transformers import BertTokenizer, BertForMaskedLM\n","from tqdm import tqdm\n","from torch.optim import AdamW\n","\n","MX_LENGTH = 50\n","BATCH_SIZE = 32\n","MASK_RATIO = 0.2\n","EPOCHS = 3\n","learning_rate = 5e-3 #5e-5 -> loss: 1~2 after one epoch"]},{"cell_type":"markdown","metadata":{"id":"SYOw7wn8_YIi"},"source":["## Prepare dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Fp3gP2nfTga","executionInfo":{"status":"aborted","timestamp":1655452456217,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"outputs":[],"source":["class BertDataset(Dataset):\n","    def __init__(self, tokenizer, max_length=512, mask_ratio=0.15):\n","        super(BertDataset, self).__init__()\n","        # self.root_dir='./data'\n","        self.train_csv=pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n","        self.tokenizer=tokenizer\n","        self.target=self.train_csv.iloc[:,1]\n","        self.max_length = max_length\n","        self.mask_ratio = mask_ratio\n","\n","    def __len__(self):\n","        return len(self.train_csv)\n","    \n","    def __getitem__(self, index):\n","        \n","        inputs = self.tokenizer.encode_plus(text=self.train_csv.iloc[index,0],\n","                                            padding='max_length',\n","                                            truncation=True,\n","                                            add_special_tokens=True,\n","                                            return_tensors='pt',\n","                                            max_length=self.max_length,)\n","\n","        return {'input_ids': inputs[\"input_ids\"].clone().detach(),\n","                'token_type_ids': inputs[\"token_type_ids\"].clone().detach(),}\n","\n","    def _apply_masking(self, x):\n","        rand = torch.rand(x['input_ids'].shape)\n","        mask = (rand < self.mask_ratio) * (x['input_ids'] != 101) * (x['input_ids'] != 102) * (x['input_ids'] != 0) # t/f tensor\n","\n","        selection = torch.flatten(mask[0].nonzero()).tolist() # idxs masked\n","        x['input_ids'][0, selection] = 103 # apply MASK token\n","\n","        return x\n","\n","    def collate_fn(self, data):\n","        batch={'input_ids':None, 'labels':None}\n","        \n","        # copy ids\n","        tmp = [item['input_ids'] for item in data]\n","        batch['labels'] = torch.stack(tmp).squeeze(1)\n","        \n","        # create mask tensor\n","        data = list(map(self._apply_masking, data))\n","        tmp = [item['input_ids'] for item in data]\n","        batch['input_ids'] = torch.stack(tmp).squeeze(1)\n","\n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXoIN3AgAlnb","executionInfo":{"status":"aborted","timestamp":1655452456217,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"outputs":[],"source":["\n","# dataset\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","dataset= BertDataset(tokenizer, max_length=MX_LENGTH, mask_ratio=MASK_RATIO)\n","\n","# dataloader\n","dataloader=DataLoader(dataset=dataset,batch_size=BATCH_SIZE, collate_fn=dataset.collate_fn)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"128XnwZsiZoD","executionInfo":{"status":"aborted","timestamp":1655452456217,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"outputs":[],"source":["#############################\n","###### Explore Dataset ######\n","#############################\n","\n","for i, batch_data in enumerate(dataloader):\n","    if i==0:\n","        print(\"data shape\")\n","        print(batch_data['labels'].shape) # batch_size * max_length\n","        print()\n","        print(\"before masking\")\n","        print(batch_data['labels'][0]) # 101: CLS, 102: SEP / we use single sentence for fine-tuning task\n","        print()\n","        print(\"after masking (MASK = 103)\")\n","        print(batch_data['input_ids'][0])"]},{"cell_type":"markdown","metadata":{"id":"xbAF5GNP_a3h"},"source":["## Prepare model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9xtPgQlh_y4","executionInfo":{"status":"aborted","timestamp":1655452456217,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"outputs":[],"source":["model = BertForMaskedLM.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbmqvJeriX95","executionInfo":{"status":"aborted","timestamp":1655452456218,"user_tz":-540,"elapsed":19,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"outputs":[],"source":["for i, batch_data in enumerate(dataloader):\n","    if i==0:\n","        print(model(**batch_data).loss)"]},{"cell_type":"markdown","metadata":{"id":"g5TYnU7D_p_J"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OC9pa0_QxYvA","executionInfo":{"status":"aborted","timestamp":1655452456218,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","model.train()\n","optim = AdamW(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1655452456218,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"},"user_tz":-540},"id":"o8yVp0yjy1j0"},"outputs":[],"source":["for epoch in range(EPOCHS):\n","    loop = tqdm(dataloader, leave=True)\n","    for batch in loop:\n","        optim.zero_grad()\n","        \n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","        \n","        outputs = model(input_ids, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optim.step()\n","\n","        loop.set_description(f'Epoch {epoch}')\n","        loop.set_postfix(loss=loss.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxusA873CoGz","executionInfo":{"status":"aborted","timestamp":1655452456218,"user_tz":-540,"elapsed":18,"user":{"displayName":"Sungwoo Coding","userId":"17899964297313081808"}}},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":["EDxtMC2g66gQ","5PgCsIyawXoN","8TqutY58tG-h","aFDRFkQTC3YQ","m3jkpgL7C7-_","Tvzu6ONF2TyE","siPUPEFXEbrD","ZDma7XS2eKSQ","SYOw7wn8_YIi","xbAF5GNP_a3h","g5TYnU7D_p_J"],"name":"lab4.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}